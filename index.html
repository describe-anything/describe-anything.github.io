<html>

<head>
    <meta charset="utf-8" />
    <title>Describe Anything: Detailed Localized Image and Video Captioning</title>

    <!-- Favicon references -->
    <link rel="icon" type="image/x-icon" href="favicon.ico">
    <link rel="icon" type="image/jpeg" href="favicon.jpg">
    <link rel="apple-touch-icon" href="favicon.jpg">

    <meta
        content="Describe Anything: Detailed Localized Image and Video Captioning"
        name="description" />
    <meta
        content="Describe Anything: Detailed Localized Image and Video Captioning"
        property="og:title" />
    <meta
        content="Describe Anything: Detailed Localized Image and Video Captioning"
        property="og:description" />
    <meta content="https://describe-anything.github.io/images/slideshow/slide1_full.jpg" property="og:image" />
    <meta
        content="Describe Anything: Detailed Localized Image and Video Captioning"
        property="twitter:title" />
    <meta
        content="Describe Anything: Detailed Localized Image and Video Captioning"
        property="twitter:description" />
    <meta content="https://describe-anything.github.io/images/slideshow/slide1_full.jpg" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?v=v3" rel="stylesheet" type="text/css" />
</head>

<body>
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="#abstract">Overview</a></li>
                    <li><a href="#task-definition">DLC Task</a></li>
                    <li><a href="#architecture">Architecture</a></li>
                    <li><a href="#data-pipeline">Data</a></li>
                    <li><a href="#dlc-bench">Benchmark</a></li>
                    <li><a href="#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero-section">
        <div class="container">
            <div class="title-row">
                <h1 class="title"><span class="gradient-text">Describe Anything</span>: Detailed Localized Image and Video Captioning</h1>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://tonylian.com" target="_blank" class="author-text">
                        Long Lian<sup>1,2</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://research.nvidia.com/person/yifan-ding" target="_blank" class="author-text">
                        Yifan Ding<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://gyhandy.github.io/" target="_blank" class="author-text">
                        Yunhao Ge<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://sifeiliu.net/" target="_blank" class="author-text">
                        Sifei Liu<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://hanzimao.me/" target="_blank" class="author-text">
                        Hanzi Mao<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://sites.google.com/site/boyilics/home" target="_blank" class="author-text">
                        Boyi Li<sup>1,2</sup>
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://research.nvidia.com/person/marco-pavone" target="_blank" class="author-text">
                        Marco Pavone<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://mingyuliu.net/" target="_blank" class="author-text">
                        Ming-Yu Liu<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank" class="author-text">
                        Trevor Darrell<sup>2</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://www.adamyala.org/" target="_blank" class="author-text">
                        Adam Yala<sup>2,3</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://ycui.me/" target="_blank" class="author-text">
                        Yin Cui<sup>1</sup>
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col affiliations">
                    <sup>1</sup>NVIDIA &nbsp;&nbsp; <sup>2</sup>UC Berkeley &nbsp;&nbsp; <sup>3</sup>UCSF
                </div>
            </div>
            <div class="link-labels base-row">
                <div class="base-col icon-col"><a href="https://arxiv.org/abs/2504.16072" target="_blank"
                        class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Paper</strong>
                    </a></div>
                <!-- <div class="base-col icon-col"><a href='https://describe-anything.github.io/'
                        class="link-block">
                        <i class="fa fa-home main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Project Page</strong>
                    </a></div> -->
                <div class="base-col icon-col"><a href='#video'
                        class="link-block">
                        <i class="fa fa-video-camera main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Video</strong>
                    </a></div>
                <div class="base-col icon-col"><a href='https://huggingface.co/spaces/longlian/describe-anything'
                        class="link-block">
                        <i class="fa fa-cube main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Interactive Demo</strong>
                    </a></div>
                <div class="base-col icon-col"><a href='https://github.com/NVlabs/describe-anything' class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Code</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Citation</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="https://github.com/NVlabs/describe-anything/tree/main/evaluation"
                        class="link-block">
                        <i class="fa fa-database main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">DLC-Bench</strong>
                    </a></div>
            </div>

        </div>
    </div>

    <main class="main-content">
        <div class="container">
            <div class="tldr">
                <b>TL;DR</b>: Our Describe Anything Model (DAM) generates detailed descriptions for user-specified regions in images and videos, marked by points, boxes, scribbles, or masks. We introduce DLC-Bench to evaluate such region-based descriptions.
            </div>
            
            <section id="additional-examples" class="section" style="margin: -1rem 0 -2rem 0;">
                <!-- <h2>High-quality Captions in Challenging Scenes</h2>

                <p>Our model is able to generate high-quality detailed localized captions for diverse regions, including the main objects, small objects in the background, and regions in cluttered scenes.</p>
                 -->
                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide">
                            <img src="images/examples/1.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A medium-sized dog with a thick, reddish-brown coat and a white underbelly and legs. The dog has a bushy tail, pointed ears, and a red collar with a silver tag. Its mouth is open, showing its teeth, and its tongue is hanging out. The dog is in a running pose with its front legs extended forward and its back legs stretched out behind.
                            </div>
                        </div>
                        <div class="slide">
                            <video src="images/examples/8.mp4" class="slideshow-image-video" autoplay muted loop></video>
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A vibrant goldfish with a striking pattern of orange and black spots glides gracefully through the water. Its body is elongated and streamlined, with a prominent dorsal fin that arches elegantly along its back. The tail fin fans out, displaying a delicate, translucent quality that ripples with each movement. As it swims, the goldfish's body undulates smoothly, propelling it forward with a gentle, rhythmic motion. The fins, particularly the pectoral fins, flutter subtly, aiding in navigation and balance. The goldfish's head is slightly pointed, with a curious, expressive eye that seems to observe its surroundings with keen interest. Throughout its journey, the goldfish maintains a steady, fluid motion, exuding a sense of calm and elegance as it interacts with the aquatic environment.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/2.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A blue wooden chair with a straight backrest featuring three vertical slats. The seat is flat and rectangular, and the chair has four sturdy legs.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/3.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A round, green metal table with a flat, circular top. The table has a cylindrical base with a horizontal black band around its middle. The base is supported by four black metal legs.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/4.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> The balcony features an ornate wrought iron railing with intricate floral and geometric patterns. The design includes symmetrical motifs with a central floral design flanked by symmetrical, ornate scrollwork. The railing is composed of vertical and horizontal bars, creating a grid-like structure with decorative elements.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/5.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A white cat with light orange ears and a pink nose. The cat has a relaxed expression with slightly closed eyes and a soft, white fur coat.
                            </div>
                        </div>
                        <div class="slide">
                            <video src="images/examples/9.mp4" class="slideshow-image-video" autoplay muted loop></video>
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A monkey with a light brown coat and a slightly darker face is seen in a series of dynamic movements. Initially, it appears to be reaching into a container with its right hand, which is holding a piece of yellow food. The monkey's posture is slightly hunched, indicating focus and intent as it interacts with the food. As the sequence progresses, the monkey brings the food closer to its mouth, using both hands to manipulate it. Its facial expression suggests concentration and enjoyment, with its eyes partially closed. The monkey's body shifts slightly, maintaining balance as it continues to eat. Throughout the sequence, the monkey's movements are fluid and purposeful, showcasing its dexterity and agility. The final frames depict the monkey holding the food with both hands, bringing it closer to its face, and then lowering it slightly, possibly preparing to take another bite.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/6.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> The eye has a dark, almost black pupil surrounded by a thick, irregularly shaped iris. The iris is predominantly light gray with patches of darker gray and brown, giving it a mottled appearance. The outer edge of the iris is slightly jagged and uneven.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/7.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A modern skyscraper with a sleek, rectangular design featuring a series of vertical, evenly spaced windows. The building has a stepped structure, with each section slightly smaller than the one below it, creating a tiered effect. The facade is predominantly composed of reflective glass panels, giving it a contemporary and polished appearance.
                            </div>
                        </div>
                        <div class="slide">
                            <video src="images/examples/10.mp4" class="slideshow-image-video" autoplay muted loop></video>
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A cow with a rich brown coat and a lighter patch on its rump is depicted in a sequence of movements. Initially, the cow is seen with its head slightly lowered, suggesting a calm demeanor. As the sequence progresses, the cow begins to move forward, its legs extending in a steady, rhythmic gait. The tail, with its tufted end, sways gently with each step, adding a sense of fluidity to its motion. The cow's body remains mostly upright, with its back slightly arched, indicating a relaxed posture. The legs, sturdy and well-defined, carry the cow forward with a sense of purpose. Throughout the sequence, the cow maintains a consistent pace, its movements smooth and unhurried, embodying a serene and composed presence.
                            </div>
                        </div>
                    </div>
                    
                    <button class="slideshow-nav prev" onclick="changeSlide(-1)">❮</button>
                    <button class="slideshow-nav next" onclick="changeSlide(1)">❯</button>
                    
                    <div class="slideshow-controls">
                        <button class="play-pause" onclick="togglePlayPause()">
                            <i class="fa fa-pause"></i>
                        </button>
                    </div>
                </div>
            </section>

            <div id="abstract" class="base-row section">
                <h2>Introducing the Describe Anything Model (DAM)</h2>
                <p class="paragraph">
                    The Describe Anything Model (DAM) is a powerful multimodal large language model that can generate detailed descriptions for specific regions in images or videos. Users can specify regions using points, boxes, scribbles, or masks, and DAM will provide rich, contextual descriptions of those regions. Watch our intro video below to learn more about DAM.
                </p>
            </div>

            <div id="video" class="base-row">
                <div class="video-container main-video-container">
                    <video class="slideshow-video align-bottom main-video" controls poster="images/slideshow/slide1_full.jpg">
                        <source src="images/video.mov" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="play-button-overlay">
                        <i class="fa fa-play"></i>
                    </div>
                    <!-- <div class="unmute-overlay" onclick="toggleMute(this)">
                        <i class="fa fa-volume-off"></i>
                        <span class="unmute-text">Click to unmute</span>
                    </div> -->
                </div>
            </div>

            

            <section id="task-definition" class="section">
                <h2>Detailed Localized Captioning (DLC)</h2>

                <p>Detailed Localized Captioning (DLC) is the task of generating comprehensive and context-aware descriptions of specific regions within an image. Unlike traditional image captioning, which summarizes the entire scene in broad strokes, DLC dives deeper into the finer details of a user-specified area. The goal is to capture not only the object's name or category but also subtle attributes such as texture, color patterns, shape, notable parts, and any visually distinctive features.</p>
                <div class="image-container">
                    <div class="image-content">
                        <img src="images/slideshow/slide1.jpg" class="img large-image">
                    </div>
                    <p class="image-caption"><strong>Describe Anything Model (DAM)</strong> performs detailed localized captioning (DLC), generating <strong>detailed</strong> and <strong>localized</strong> descriptions for user-specified regions within <em>images</em>. DAM accepts various user inputs for region specification, including clicks, scribbles, boxes, and masks.<br/> <span class="image-credit">(Image credit: Markus Trienke (CC BY-SA 2.0))</span></p>
                </div>

                <p>DLC extends naturally to videos by describing how a specified region's appearance and context change over time. Models must track the target across frames, capturing evolving attributes, interactions, and subtle transformations.</p>
                <div class="image-container">
                    <video class="slideshow-video align-bottom" autoplay muted loop poster="images/slideshow/slide2.jpg">
                        <source src="images/video_example.mov" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="image-caption"><strong>Describe Anything Model (DAM)</strong> can also perform detailed localized video captioning, describing how a specified region changes over time. For localized <em>video</em> descriptions, specifying the region on <em>only a single frame</em> is sufficient. See our paper for full descriptions.<br/> <span class="image-credit">(Video credit: MOSE Dataset (CC BY-NC-SA 4.0))</span></p>
                </div>
            </section>

            <section id="highly-detailed-captioning" class="section">
                <h2>Highly Detailed Image and Video Captioning</h2>

                <p>Our method excels at producing detailed descriptions of objects in both images and videos. By balancing the clarity of a focal region with global context, the model can highlight subtle features—like intricate patterns or changing textures—far beyond what general image-level captioning provides.</p>
                <div class="image-container">
                    <img src="images/slideshow/slide3.jpg"
                        class="image-item img large-image z-depth-1">
                    <p class="image-caption">Compared to prior works, the description from our <strong>Describe Anything Model (DAM)</strong> is more detailed and accurate. <span class="image-credit">(Image credit: SAM Materials (CC BY-SA 4.0))</span></p>
                </div>
            </section>

            <section id="instruction-controlled-captioning" class="section">
                <h2>Instruction-controlled Captioning</h2>

                <p>Users can guide our model to produce descriptions of varying detail and style. Whether a brief summary or a long, intricate narrative is needed, the model can adapt its output. This flexibility benefits diverse use cases, from rapid labeling tasks to in-depth expert analyses.</p>
                <div class="image-container">
                    <img src="images/slideshow/slide4.jpg"
                        class="image-item img large-image z-depth-1">
                    <p class="image-caption"><strong>Describe Anything Model (DAM)</strong> can generate descriptions of varying detail and style according to user instructions.<br/> <span class="image-credit">(Image credit: SAM Materials (CC BY-SA 4.0))</span></p>
                </div>
            </section>

            <section id="zero-shot-regional-qa" class="section">
                <h2>Zero-shot Regional QA</h2>

                <p>Beyond descriptions, our model can answer questions about a specified region without extra training data. Users can ask about the region's attributes, and the model draws on its localized understanding to provide accurate, context-driven answers. This capability enhances natural, interactive use cases.</p>
                <div class="image-container">
                    <img src="images/slideshow/slide5.jpg"
                        class="image-item img large-image z-depth-1">
                </div>
            </section>

            <section id="architecture" class="section">
                <h2>Architecture of Describe Anything Model (DAM)</h2>

                <p>Our architecture uses a "Focal Prompt" to provide both the full image and a zoomed-in view of the target region. This approach ensures the model sees fine details while retaining global context. The result is detailed, accurate captions that reflect both the bigger picture and the smallest nuances.</p>
                <div class="image-container">
                    <img src="images/slideshow/slide6.jpg"
                        class="image-item img large-image z-depth-1">
                    <p class="image-caption"><span class="image-credit">Image credit: Objects365 Dataset (CC BY 4.0)</span></p>
                </div>
                
                <p>We introduce a localized vision backbone that integrates global and focal features. Images and masks are aligned spatially, and gated cross-attention layers fuse detailed local cues with global context. New parameters are initialized to zero, preserving pre-trained capabilities. This design yields richer, more context-aware descriptions.</p>
                <div class="image-container" style="display: flex; justify-content: center;">
                    <img src="images/slideshow/slide7.jpg"
                        class="image-item img large-image z-depth-1" style="max-width: 500px; width: 50%;">
                </div>
            </section>

            <section id="data-pipeline" class="section">
                <h2>Semi-supervised Data Pipeline for Detailed Localized Captioning (DLC-SDP)</h2>

                <p>Because existing datasets lack detailed localized descriptions, we devised a two-stage pipeline. First, we use a VLM to expand short class labels from segmentation datasets into rich descriptions. Second, we apply self-training as a form of semi-supervised learning on unlabeled images, using our model to generate and refine new captions. This scalable approach builds large, high-quality training data without relying on extensive human annotation.</p>
                <div class="image-container">
                    <img src="images/slideshow/slide8.jpg"
                        class="image-item img large-image z-depth-1">
                </div>
            </section>

            <section id="dlc-bench" class="section">
                <h2>DLC-Bench: A Benchmark for Detailed Localized Captioning</h2>

                <p>We introduce DLC-Bench, a benchmark that uses an LLM-based judge to evaluate a model's region-based descriptions. Instead of relying on simple text overlap, DLC-Bench checks for correct details and the absence of errors. This offers a more accurate and human-like metric for measuring DLC performance.</p>
                <div class="image-container">
                    <img src="images/slideshow/slide9.jpg"
                        class="image-item img large-image z-depth-1">
                    <p class="image-caption">In DLC-Bench, a captioning model is prompted to describe a specified image region. The generated description is then evaluated by querying an LLM Judge. Points are assigned or deducted based on the LLM's response. The question we show is an example of positive questions.<br/> <span class="image-credit">(Image credit: Objects365 Dataset (CC BY 4.0))</span></p>
                </div>
            </section>

            <section id="table-advantages" class="section">
                <h2>Advantages of DAM, DLC-SDP, and DLC-Bench</h2>
                <div class="table-container">
                    <table class="advantages-table">
                        <thead>
                            <tr>
                                <th>Component</th>
                                <th>Previous Practice</th>
                                <th>Problem</th>
                                <th>Our Solution</th>
                                <th>Advantages</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Describe Anything Model (DAM)</strong></td>
                                <td>Extracting regional features from global image features</td>
                                <td>Regional details already lost in image feature extraction and not provided to the LLM</td>
                                <td>Providing <strong>focal prompt</strong> to proposed <strong>localized vision backbone</strong></td>
                                <td>Detail-rich contextful features allowing for accurate, multi-granular localized descriptions</td>
                            </tr>
                            <tr>
                                <td rowspan="2"><strong>SSL Data Pipeline (DLC-SDP)</strong></td>
                                <td>Query a data curation VLM with referring <strong>boxes</strong> and <strong>global image captions</strong></td>
                                <td>Imprecise referring to data curation model</td>
                                <td>Reframe the query into a <strong>mask</strong>-referred <strong>keyword</strong> expansion question</td>
                                <td>Leverage high-quality precise human annotated regional masks and keywords</td>
                            </tr>
                            <tr>
                                <td><strong>Fully supervised learning</strong></td>
                                <td>Limited data with high annotation quality</td>
                                <td><strong>Semi-supervised learning</strong></td>
                                <td>Scalable to diverse web-scale unlabeled datasets</td>
                            </tr>
                            <tr>
                                <td><strong>Benchmark (DLC-Bench)</strong></td>
                                <td>Pred caption + <strong>reference GT caption</strong> → language-based similarity metrics or LLM scorer</td>
                                <td>Incorrect hallucination penalty for correct details not present in the reference caption</td>
                                <td>Pred caption + <strong>query for positive/negative attributes</strong> → LLM scorer</td>
                                <td>Accurate detail and hallucination assessment without relying on reference captions</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <section id="comparison" class="section">
                <h2>Comparison</h2>

                <p>On DLC-Bench, our model outperforms existing solutions by producing more detailed and accurate localized descriptions with less hallucination. It surpasses models trained for general image-level tasks and those designed for localized reasoning, setting a new standard for detailed, context-rich captioning.</p>

                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide">
                            <img src="images/results/1.png" class="comparison-slideshow-image">
                            <div class="image-caption">
                                <b>Accuracies on detailed localized captioning in our proposed DLC-Bench.</b> DAM outperforms previous API-only models, open-source models, and region-specific VLMs on detailed localized captioning. <span class="image-credit"><u>Underlined</u>: the second-best method. †: models with thinking mode.</span>
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/results/2.png" class="comparison-slideshow-image">
                            <div class="image-caption">
                                <b>Detailed localized video captioning on HC-STVG.</b> DAM outperforms previous models on detailed localized video captioning. <span class="image-credit">†: models with thinking mode.</span>
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/results/3.png" class="comparison-slideshow-image">
                            <div class="image-caption">
                                <b>Performance on detailed localized video description on VideoRefer-Bench-D.</b> †: We provide analysis on hallucination scores (HD) in our quantitative results and reference captions sections. *: trained on in-domain VideoRefer-700k with regard to VideoRefer-Bench, both sourcing videos from Panda-70M.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/results/4.png" class="comparison-slideshow-image">
                            <div class="image-caption">
                                LVIS and PACO open-class <b>keyword-level</b> captioning benchmarks. DAM excels particularly in the challenging PACO benchmark that requires distinguishing between objects and parts.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/results/5.png" class="comparison-slideshow-image">
                            <div class="image-caption">
                                Zero-shot evaluation on <b>phrase-level</b> dataset Flickr30k Entities. Our model achieves 7.34% average relative improvement against previous best.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/results/6.png" class="comparison-slideshow-image">
                            <div class="image-caption">
                                Zero-shot evaluation on the <b>detailed captioning</b> dataset Ref-L4. Our method achieves 39.5% and 13.1% average relative improvement on the short/long language-based captioning metrics, respectively.
                            </div>
                        </div>
                    </div>
                    
                    <button class="slideshow-nav prev" onclick="changeSlide(-1)">❮</button>
                    <button class="slideshow-nav next" onclick="changeSlide(1)">❯</button>
                    
                    <div class="slideshow-controls">
                        <button class="play-pause" onclick="togglePlayPause()">
                            <i class="fa fa-pause"></i>
                        </button>
                    </div>
                </div>
            </section>

            <section id="conclusion" class="section">
                <h2>Conclusion</h2>

                <p>Our <strong>Describe Anything Model (DAM)</strong> generates detailed descriptions for specified regions in images and videos, and can be utilized in various applications ranging from data annotation to serving as an intermediate component in downstream tasks. We plan to publicly release our code, models, data, and benchmark to support future research. We are excited to see the community explore the potential of detailed localized captioning. We hope this benchmark and model will serve as a useful resource for future research in this area.</p>
            </section>

            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0">@article{lian2025describe,
    title={Describe Anything: Detailed Localized Image and Video Captioning}, 
    author={Lian, Long and Ding, Yifan and Ge, Yunhao and Liu, Sifei and Mao, Hanzi and Li, Boyi and Pavone, Marco and Liu, Ming-Yu and Darrell, Trevor and Yala, Adam and Cui, Yin},
    journal={arXiv preprint},
    year={2025}
}</pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a>.</p>
        </div>
    </footer>

    <script>
    function toggleMute(element) {
        const video = element.parentElement.querySelector('video');
        const icon = element.querySelector('i');
        const text = element.querySelector('.unmute-text');
        
        if (video.muted) {
            video.muted = false;
            icon.className = 'fa fa-volume-up';
            text.textContent = 'Mute';
        } else {
            video.muted = true;
            icon.className = 'fa fa-volume-off';
            text.textContent = 'Click to unmute';
        }
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.querySelectorAll('video');
        videos.forEach(video => {
            video.addEventListener('play', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
            
            video.addEventListener('pause', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
        });

        // Initialize all slideshows
        document.querySelectorAll('.slideshow-container').forEach(container => {
            const slideshow = container.querySelector('.slideshow');
            const slides = slideshow.querySelectorAll('.slide');
            const prevButton = container.querySelector('.slideshow-nav.prev');
            const nextButton = container.querySelector('.slideshow-nav.next');
            const playPauseButton = container.querySelector('.play-pause');
            
            let currentSlide = 0;
            let autoplayInterval;
            let isPlaying = true;

            function showSlide(n) {
                slides.forEach(slide => slide.classList.remove('active'));
                currentSlide = (n + slides.length) % slides.length;
                slides[currentSlide].classList.add('active');
            }

            function changeSlide(n) {
                showSlide(currentSlide + n);
                resetAutoplay();
            }

            function togglePlayPause() {
                if (isPlaying) {
                    clearInterval(autoplayInterval);
                    playPauseButton.innerHTML = '<i class="fa fa-play"></i>';
                } else {
                    startAutoplay();
                    playPauseButton.innerHTML = '<i class="fa fa-pause"></i>';
                }
                isPlaying = !isPlaying;
            }

            function startAutoplay() {
                autoplayInterval = setInterval(() => {
                    showSlide(currentSlide + 1);
                }, 5000);
            }

            function resetAutoplay() {
                clearInterval(autoplayInterval);
                if (isPlaying) {
                    startAutoplay();
                }
            }

            // Initialize this slideshow
            showSlide(0);
            startAutoplay();

            // Add event listeners
            prevButton.addEventListener('click', () => changeSlide(-1));
            nextButton.addEventListener('click', () => changeSlide(1));
            playPauseButton.addEventListener('click', togglePlayPause);
        });

        // Handle main video play button
        const mainVideo = document.querySelector('.main-video');
        const playButton = document.querySelector('.play-button-overlay');
        
        if (mainVideo && playButton) {
            // Click play button to play video
            playButton.addEventListener('click', () => {
                mainVideo.play();
                mainVideo.classList.add('playing');
            });

            // Handle video play/pause events
            mainVideo.addEventListener('play', () => {
                mainVideo.classList.add('playing');
            });

            mainVideo.addEventListener('pause', () => {
                mainVideo.classList.remove('playing');
            });

            mainVideo.addEventListener('ended', () => {
                mainVideo.classList.remove('playing');
            });
        }
    });
    </script>
</body>
</html>